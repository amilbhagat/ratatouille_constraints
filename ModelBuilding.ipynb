{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parul21065/testTwo/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# --optional (to debug the cuda error)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path='train_temp', block_size=512): \n",
    "        cached_features_file = \"data_temp.h5\"\n",
    "\n",
    "        # logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        print((\"Loading features from cached file %s\", cached_features_file))\n",
    "        with h5py.File(cached_features_file, 'r') as f:\n",
    "            if file_path=='test_temp':\n",
    "                self.samples = f[file_path][:] #this is a dev set, 30% of a test set\n",
    "            else:\n",
    "                self.samples = f[file_path][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.samples[item])\n",
    "\n",
    "def get_dataset( tokenizer, evaluate=False, local_rank=-1):\n",
    "  file_path = \"test_temp\" if evaluate else \"train_temp\"\n",
    "  return H5Dataset(tokenizer=tokenizer, file_path=file_path)\n",
    "\n",
    "set_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('gpt2', cache_dir='cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', cache_dir= 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parul21065/testTwo/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\n",
    "            'gpt2', # model name\n",
    "            config=config,\n",
    "            cache_dir='cache', # cache directory (path to the cache directory)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"additional_special_tokens\": ['<RECIPE_START>',\n",
    "                                  '<INPUT_START>',\n",
    "                                  '<NEXT_INPUT>',\n",
    "                                  '<INPUT_END>',\n",
    "                                  '<INGR_START>',\n",
    "                                  '<NEXT_INGR>',\n",
    "                                  '<INGR_END>',\n",
    "                                  '<INSTR_START>',\n",
    "                                  '<NEXT_INSTR>',\n",
    "                                  '<INSTR_END>',\n",
    "                                  '<TITLE_START>'\n",
    "                                  ,'<TITLE_END>'\n",
    "                                  ,'<RECIPE_END>'\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50270, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loading features from cached file %s', 'data_temp.h5')\n",
      "('Loading features from cached file %s', 'data_temp.h5')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ( get_dataset(tokenizer=tokenizer) )\n",
    "eval_dataset = (  get_dataset(tokenizer=tokenizer, evaluate=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False, mlm_probability=0.15  )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    output_dir= \"./outputs\",\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    fp16_opt_level='O1',\n",
    "    warmup_steps=1e2,    \n",
    "    learning_rate=5e-4,\n",
    "    adam_epsilon=1e-8,\n",
    "    weight_decay=0.01,        \n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./outputs/tempt/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/tempt/special_tokens_map.json\n",
      "/home/parul21065/testTwo/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 35762\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1117' max='1117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1117/1117 48:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.090800</td>\n",
       "      <td>1.551185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.554700</td>\n",
       "      <td>1.466045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1513\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./outputs/checkpoint-500\n",
      "Configuration saved in ./outputs/checkpoint-500/config.json\n",
      "Model weights saved in ./outputs/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1513\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./outputs/checkpoint-1000\n",
      "Configuration saved in ./outputs/checkpoint-1000/config.json\n",
      "Model weights saved in ./outputs/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./outputs/checkpoint-1000 (score: 1.4660452604293823).\n",
      "Saving model checkpoint to ./outputs\n",
      "Configuration saved in ./outputs/config.json\n",
      "Model weights saved in ./outputs/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./outputs/tempt')\n",
    "# Starting the Training and saving the model\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
